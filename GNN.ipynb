{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import  pickle as pkl\n",
    "import networkx as nx"
   ]
  },
  {
   "source": [
    "# DataLoad\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"cora\"\n",
    "def preprocess_features(features):\n",
    "    \"\"\"\n",
    "    行归一化 每行除以每行的和\n",
    "    \"\"\"\n",
    "    row_sum = np.array(features.sum(1))\n",
    "    reverse_row_sum = np.power(row_sum, -1).flatten()\n",
    "    reverse_row_sum[np.isinf(reverse_row_sum)] = 0.\n",
    "    new_features = sp.diags(reverse_row_sum).dot(features)\n",
    "    return new_features\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "    dataset_name = cora, citeseet, pubmed\n",
    "    # 都是pickle文件\n",
    "    .x: 训练集的feature vectors, scipy.sparse.csr.csr_matrix\n",
    "    .tx: 测试集feature vectors\n",
    "    .allx: labeled 和unlabeled training feature vectors\n",
    "    .y: one-hot labels of .x\n",
    "    .ty: one-hot labels of .tx\n",
    "    .ally: one-hot labels of .allx\n",
    "    .graph: {index: [index_of_neighbor_nodes]}\n",
    "    .test.index: 测试实例的id\n",
    "    \"\"\"\n",
    "    names = ['x', 'tx', 'allx','y','ty','ally', 'graph']\n",
    "    objects = {}\n",
    "    for name in names:\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_name, name), 'rb') as f:\n",
    "            objects[name] = pkl.load(f, encoding='latin1')\n",
    "\n",
    "    # 测试节点的id\n",
    "    with open(\"data/ind.{}.test.index\".format(dataset_name), 'r') as f:\n",
    "        index = []\n",
    "        for line in f.readlines():\n",
    "            index.append(int(line.strip()))\n",
    "        \n",
    "    test_index_reorder = np.sort(index)  # 测试集的ids\n",
    "    print(objects['x'].shape)   # (140,1433)    训练集中带有标签的部分\n",
    "    print(objects['tx'].shape)  # (1000, 1433)\n",
    "    print(objects['allx'].shape)  # (1708, 1433)  训练集\n",
    "    print(len(test_index_reorder))  # 1000\n",
    "    \n",
    "    \n",
    "    whole_features = sp.vstack((objects['allx'], objects['tx'])).tolil()  # tx 的顺序是乱序的 index 转为lil稀疏矩阵 可以直接运算\n",
    "    \n",
    "\n",
    "\n",
    "    whole_features[index,:] = whole_features[test_index_reorder,:]\n",
    "\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(objects['graph']))  # return a scipy sparse matrix\n",
    "\n",
    "    whole_labels = np.r_[objects['ally'], objects['ty']]\n",
    "    whole_labels[index,:] = whole_labels[test_index_reorder,:]  \n",
    "\n",
    "    whole_features = preprocess_features(whole_features)\n",
    "    # 训练集featrues\n",
    "\n",
    "    X_train = whole_features[range(objects['x'].shape[0])]\n",
    "    y_train = whole_labels[range(objects['y'].shape[0])]\n",
    "\n",
    "    X_valid = whole_features[objects['x'].shape[0]: objects['x'].shape[0] + 500]\n",
    "    y_valid = whole_labels[objects['y'].shape[0]: objects['y'].shape[0] + 500]\n",
    "\n",
    "    X_test = whole_features[test_index_reorder]\n",
    "    y_test = whole_labels[test_index_reorder]\n",
    "\n",
    "    return adj, whole_features ,X_train, X_valid, X_test, y_train, y_valid, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(140, 1433)\n(1000, 1433)\n(1708, 1433)\n1000\n"
     ]
    }
   ],
   "source": [
    "adj, whole_features, X_train, X_valid, X_test, y_train, y_valid, y_test = load_data(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2708, 2708)\n<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(adj.shape)\n",
    "print(type(adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2708, 1433)\n<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(whole_features.shape)\n",
    "print(type(whole_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(140, 1433)\n(140, 7)\n(500, 1433)\n(500, 7)\n(1000, 1433)\n(1000, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "source": [
    "## 计算拉普拉斯矩阵"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node = adj.shape[0]\n",
    "\n",
    "def graph_laplacian(adj):\n",
    "    \"\"\"\n",
    "    adj: sparse.csr_matrix\n",
    "    \"\"\"\n",
    "    adj = adj + sp.eye(num_node)\n",
    "    D_ii = np.power(np.array(adj.sum(axis = 1)).flatten(), -0.5)\n",
    "    D_hat = sp.diags(D_ii)\n",
    "    lapacian = D_hat @ adj @ D_hat\n",
    "    return lapacian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "graph_laplacian = graph_laplacian(adj)\n",
    "type(graph_laplacian)"
   ]
  },
  {
   "source": [
    "# Build GCN Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 定义GCN的层次\n",
    "\n",
    "其中: dropout rate for all layers, L2 regularization factor for the first GCN layer and number of hiddenunits"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphCNN(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, \n",
    "                       output_dim, \n",
    "                       sparse_inputs = False,\n",
    "                       dropout = 0.,\n",
    "                       bias = False,\n",
    "                       activation = keras.activations.relu, \n",
    "                       **kwargs):\n",
    "        super(GraphCNN,self).__init__(**kwargs) \n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.biases = bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        定义层内参数\n",
    "        input_shape (features, graphLaplacian)\n",
    "        \"\"\"\n",
    "        self.Weight_per_layer = self.add_weight(name = \"weight\",\n",
    "                                                shape = (input_shape[0][1], self.output_dim),\n",
    "                                                initializer = keras.initializers.glorot_uniform,\n",
    "                                                trainable = True)\n",
    "        if self.biases:\n",
    "            self.bias_per_layer = self.add_weight(name = \"bias\",\n",
    "                                                  shape = (input_shape[0][1],),\n",
    "                                                  initializer = keras.initializers.zeros,\n",
    "                                                  trainable = True)\n",
    "        super(GraphCNN,self).build(input_shape)\n",
    "\n",
    "\n",
    "    def _sparse_csr_matrix_to_TFSparseTensor(self, X):\n",
    "        if sp.isspmatrix_csr(X):\n",
    "            X = X.tocoo()\n",
    "        row = X.row\n",
    "        col = X.col\n",
    "        pos = np.c_[row,col]\n",
    "        data = X.data\n",
    "        shape = X.shape\n",
    "        # SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n",
    "        return tf.SparseTensor(pos, data, shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        x: (features, GraphLaplacian)\n",
    "        dropout before input to next layer\n",
    "        features is a SparseTensor\n",
    "        GraphLaplacian is a csr_matrix\n",
    "        \"\"\"\n",
    "        features, GraphLaplacian = inputs\n",
    "        \n",
    "        # if training is not False and self.sparse_inputs:\n",
    "        #     # drop out\n",
    "        #     features = None\n",
    "        # elif training is not False:\n",
    "        #     features = tf.nn.dropout(x, self.dropout)\n",
    "        # print(type(features))\n",
    "        # print(type(GraphLaplacian))\n",
    "        L_dot_X = list()\n",
    "        if self.sparse_inputs:\n",
    "            for i in GraphLaplacian:\n",
    "                L_dot_X.append(tf.sparse.sparse_dense_matmul(np.array(i.todense()).astype('float32'), features))\n",
    "        else:\n",
    "            for i in GraphLaplacian:\n",
    "                L_dot_X.append(tf.matmul(np.array(i.todense()).astype('float32'), features)) \n",
    "\n",
    "        return self.activation(L_dot_X @ self.Weight_per_layer)\n",
    "\n",
    "        # # 加快运行\n",
    "        # L_dot_X = []\n",
    "        # for i in GraphLaplacian:\n",
    "            \n",
    "        #     L_dot_X.append(tf.sparse.sparse_dense_matmul(i.todense(), features))\n",
    "        \n",
    "        # return self.activation(tf.matmul(np.array(L_dot_X), self.Weight_per_layer))\n",
    "\n",
    "\n",
    "\n",
    "class Sparse_Dropout(keras.layers.Layer):\n",
    "    def __init__(self, rate, noise_shape, **kwargs):\n",
    "        \"\"\"\n",
    "        一部分feature 置为0\n",
    "        rate: dropout rate\n",
    "        noise: num of nonzero features\n",
    "        \"\"\"\n",
    "        super(Sparse_Dropout,self).__init__()\n",
    "        self.rate = rate\n",
    "        self.noise_shape = noise_shape  \n",
    "\n",
    "    def build(self, input_shape):\n",
    "         super(Sparse_Dropout,self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        # input is a tf.Sparse Tensor\n",
    "        if training:\n",
    "            retain_prob = tf.random.uniform(shape=[self.noise_shape[0]])   # 每个feature被保留的概率\n",
    "            retain = (retain_prob >= self.rate)\n",
    "            pre_out = tf.sparse.retain(inputs, retain)\n",
    "            return pre_out * (1./ (1-self.rate))\n",
    "        else: \n",
    "            return inputs\n"
   ]
  },
  {
   "source": [
    "## 定义模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(keras.models.Model):\n",
    "    def __init__(self, input_dim, output_dim,  hidden_dim, num_nonzero_feature, sparse_input = True, dropout = 0.,**kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        print('input dim:', input_dim)\n",
    "        print('output dim:', output_dim)\n",
    "        print('num_features_nonzero:', num_nonzero_feature)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sparse_input = sparse_input\n",
    "        self.dropout = dropout\n",
    "        self.dropout1 = Sparse_Dropout(rate = self.dropout, noise_shape = num_nonzero_feature)\n",
    "        self.GraphCNN1 = GraphCNN(input_dim = self.input_dim, \n",
    "                                 output_dim = self.hidden_dim, \n",
    "                                 sparse_inputs = self.sparse_input, \n",
    "                                 dropout = self.dropout,\n",
    "                                 activation = keras.activations.relu)\n",
    "        self.dropout2 = keras.layers.Dropout(self.dropout)\n",
    "        self.GraphCNN2 = GraphCNN(input_dim = self.hidden_dim, \n",
    "                                 output_dim = self.output_dim, \n",
    "                                 dropout = self.dropout,\n",
    "                                 activation = keras.activations.softmax)\n",
    "\n",
    "        # self.activation = keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        features, graph_laplacian = inputs\n",
    "        dropout1 = self.dropout1(features,training = training) # output(2708, 1433)\n",
    "        gcn1 = self.GraphCNN1((dropout1,graph_laplacian))\n",
    "        dropout2 = tf.squeeze(self.dropout2(gcn1))                    # (2708, 1, 16)\n",
    "        gcn2 = self.GraphCNN2((dropout2,graph_laplacian))\n",
    "        # out = self.activation(gcn2)\n",
    "        return tf.squeeze(gcn2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sp.sparse.csr_matrix to tf.SparseTensor\n",
    "def sparse_csr_matrix_to_TFSparseTensor(X):\n",
    "    if sp.isspmatrix_csr(X):\n",
    "        X = X.tocoo()\n",
    "    row = X.row\n",
    "    col = X.col\n",
    "    pos = np.c_[row,col]\n",
    "    data = X.data\n",
    "    shape = X.shape\n",
    "    # SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n",
    "    return tf.SparseTensor(pos, data, shape)\n",
    "\n",
    "features = sparse_csr_matrix_to_TFSparseTensor(whole_features) \n",
    "num_nonzero_value = features.values.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "49216"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "num_nonzero_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input dim: 1433\noutput dim: 7\nnum_features_nonzero: [49216]\n"
     ]
    }
   ],
   "source": [
    "gcn_model = GCN(input_dim = features.shape[1], \n",
    "                output_dim = y_train.shape[1], \n",
    "                hidden_dim = 16, \n",
    "                num_nonzero_feature = num_nonzero_value,\n",
    "                dropout = 0.5)"
   ]
  },
  {
   "source": [
    "## 定义loss"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  0  train loss:  1.9462742805480957  train acc:  0.10000000149011612  val loss:  1.9434977769851685  val acc:  0.3319999873638153\n",
      "epoch:  1  train loss:  1.9410849809646606  train acc:  0.37142857909202576  val loss:  1.9400094747543335  val acc:  0.4440000057220459\n",
      "epoch:  2  train loss:  1.9356205463409424  train acc:  0.5214285850524902  val loss:  1.9356262683868408  val acc:  0.5099999904632568\n",
      "epoch:  3  train loss:  1.9281036853790283  train acc:  0.6285714507102966  val loss:  1.9306448698043823  val acc:  0.5619999766349792\n",
      "epoch:  4  train loss:  1.9195727109909058  train acc:  0.6571428775787354  val loss:  1.9253590106964111  val acc:  0.5879999995231628\n",
      "epoch:  5  train loss:  1.910069227218628  train acc:  0.699999988079071  val loss:  1.919954538345337  val acc:  0.5899999737739563\n",
      "epoch:  6  train loss:  1.9002541303634644  train acc:  0.7142857313156128  val loss:  1.9143290519714355  val acc:  0.593999981880188\n",
      "epoch:  7  train loss:  1.8874832391738892  train acc:  0.7285714149475098  val loss:  1.9084724187850952  val acc:  0.5960000157356262\n",
      "epoch:  8  train loss:  1.8782954216003418  train acc:  0.7642857432365417  val loss:  1.9022997617721558  val acc:  0.6060000061988831\n",
      "epoch:  9  train loss:  1.8707940578460693  train acc:  0.7714285850524902  val loss:  1.8958073854446411  val acc:  0.6159999966621399\n",
      "epoch:  10  train loss:  1.8560646772384644  train acc:  0.7642857432365417  val loss:  1.8889003992080688  val acc:  0.6259999871253967\n",
      "epoch:  11  train loss:  1.8412165641784668  train acc:  0.7928571701049805  val loss:  1.8817155361175537  val acc:  0.6299999952316284\n",
      "epoch:  12  train loss:  1.8266997337341309  train acc:  0.8214285969734192  val loss:  1.8741455078125  val acc:  0.6299999952316284\n",
      "epoch:  13  train loss:  1.820194959640503  train acc:  0.7928571701049805  val loss:  1.8662441968917847  val acc:  0.628000020980835\n",
      "epoch:  14  train loss:  1.7936210632324219  train acc:  0.8214285969734192  val loss:  1.8580000400543213  val acc:  0.6320000290870667\n",
      "epoch:  15  train loss:  1.7890923023223877  train acc:  0.8571428656578064  val loss:  1.8494631052017212  val acc:  0.6299999952316284\n",
      "epoch:  16  train loss:  1.7724215984344482  train acc:  0.8714285492897034  val loss:  1.840634822845459  val acc:  0.6359999775886536\n",
      "epoch:  17  train loss:  1.7579723596572876  train acc:  0.8285714387893677  val loss:  1.8314340114593506  val acc:  0.6439999938011169\n",
      "epoch:  18  train loss:  1.7460134029388428  train acc:  0.8071428537368774  val loss:  1.8218600749969482  val acc:  0.6499999761581421\n",
      "epoch:  19  train loss:  1.7185434103012085  train acc:  0.8500000238418579  val loss:  1.811893343925476  val acc:  0.6539999842643738\n",
      "epoch:  20  train loss:  1.7009540796279907  train acc:  0.8214285969734192  val loss:  1.801523208618164  val acc:  0.656000018119812\n",
      "epoch:  21  train loss:  1.6842235326766968  train acc:  0.8428571224212646  val loss:  1.7907508611679077  val acc:  0.6620000004768372\n",
      "epoch:  22  train loss:  1.6677896976470947  train acc:  0.8571428656578064  val loss:  1.7797406911849976  val acc:  0.6660000085830688\n",
      "epoch:  23  train loss:  1.6395429372787476  train acc:  0.7928571701049805  val loss:  1.768179178237915  val acc:  0.6700000166893005\n",
      "epoch:  24  train loss:  1.6226798295974731  train acc:  0.8714285492897034  val loss:  1.7563201189041138  val acc:  0.6700000166893005\n",
      "epoch:  25  train loss:  1.6014841794967651  train acc:  0.8500000238418579  val loss:  1.7441620826721191  val acc:  0.6679999828338623\n",
      "epoch:  26  train loss:  1.6022653579711914  train acc:  0.8571428656578064  val loss:  1.7316313982009888  val acc:  0.6800000071525574\n",
      "epoch:  27  train loss:  1.5576649904251099  train acc:  0.8714285492897034  val loss:  1.7186495065689087  val acc:  0.6819999814033508\n",
      "epoch:  28  train loss:  1.5345020294189453  train acc:  0.8857142925262451  val loss:  1.7054404020309448  val acc:  0.6840000152587891\n",
      "epoch:  29  train loss:  1.511748194694519  train acc:  0.8785714507102966  val loss:  1.6918762922286987  val acc:  0.6880000233650208\n",
      "epoch:  30  train loss:  1.5001277923583984  train acc:  0.8714285492897034  val loss:  1.677823781967163  val acc:  0.6899999976158142\n",
      "epoch:  31  train loss:  1.482301950454712  train acc:  0.8785714507102966  val loss:  1.6634489297866821  val acc:  0.699999988079071\n",
      "epoch:  32  train loss:  1.4424664974212646  train acc:  0.8999999761581421  val loss:  1.6488507986068726  val acc:  0.7039999961853027\n",
      "epoch:  33  train loss:  1.4304027557373047  train acc:  0.8857142925262451  val loss:  1.633870005607605  val acc:  0.7099999785423279\n",
      "epoch:  34  train loss:  1.397632360458374  train acc:  0.8642857074737549  val loss:  1.6187230348587036  val acc:  0.7099999785423279\n",
      "epoch:  35  train loss:  1.36167573928833  train acc:  0.8857142925262451  val loss:  1.603279948234558  val acc:  0.7160000205039978\n",
      "epoch:  36  train loss:  1.3396883010864258  train acc:  0.8785714507102966  val loss:  1.587743878364563  val acc:  0.7200000286102295\n",
      "epoch:  37  train loss:  1.302268385887146  train acc:  0.9214285612106323  val loss:  1.5719797611236572  val acc:  0.7239999771118164\n",
      "epoch:  38  train loss:  1.2941043376922607  train acc:  0.8857142925262451  val loss:  1.5557982921600342  val acc:  0.7279999852180481\n",
      "epoch:  39  train loss:  1.298154354095459  train acc:  0.9142857193946838  val loss:  1.5393296480178833  val acc:  0.7279999852180481\n",
      "epoch:  40  train loss:  1.2344025373458862  train acc:  0.9285714030265808  val loss:  1.522390604019165  val acc:  0.7279999852180481\n",
      "epoch:  41  train loss:  1.1991671323776245  train acc:  0.9428571462631226  val loss:  1.5053062438964844  val acc:  0.7300000190734863\n",
      "epoch:  42  train loss:  1.1912628412246704  train acc:  0.8642857074737549  val loss:  1.4880385398864746  val acc:  0.7300000190734863\n",
      "epoch:  43  train loss:  1.1289856433868408  train acc:  0.9142857193946838  val loss:  1.4703829288482666  val acc:  0.7300000190734863\n",
      "epoch:  44  train loss:  1.129357099533081  train acc:  0.9285714030265808  val loss:  1.452714204788208  val acc:  0.7319999933242798\n",
      "epoch:  45  train loss:  1.1130037307739258  train acc:  0.9071428775787354  val loss:  1.4350545406341553  val acc:  0.7360000014305115\n",
      "epoch:  46  train loss:  1.0655089616775513  train acc:  0.9142857193946838  val loss:  1.4175548553466797  val acc:  0.7379999756813049\n",
      "epoch:  47  train loss:  1.0890533924102783  train acc:  0.8999999761581421  val loss:  1.4000064134597778  val acc:  0.7440000176429749\n",
      "epoch:  48  train loss:  1.0562846660614014  train acc:  0.9285714030265808  val loss:  1.3824841976165771  val acc:  0.7459999918937683\n",
      "epoch:  49  train loss:  1.0205650329589844  train acc:  0.9142857193946838  val loss:  1.364905595779419  val acc:  0.7459999918937683\n",
      "epoch:  50  train loss:  1.003780484199524  train acc:  0.9071428775787354  val loss:  1.3474678993225098  val acc:  0.7480000257492065\n",
      "epoch:  51  train loss:  0.948902428150177  train acc:  0.9214285612106323  val loss:  1.3299108743667603  val acc:  0.75\n",
      "epoch:  52  train loss:  0.9680356979370117  train acc:  0.9357143044471741  val loss:  1.312829613685608  val acc:  0.7519999742507935\n",
      "epoch:  53  train loss:  0.9441404342651367  train acc:  0.9214285612106323  val loss:  1.2959073781967163  val acc:  0.7519999742507935\n",
      "epoch:  54  train loss:  0.8811538219451904  train acc:  0.9357143044471741  val loss:  1.279167652130127  val acc:  0.7519999742507935\n",
      "epoch:  55  train loss:  0.8891952633857727  train acc:  0.949999988079071  val loss:  1.2623800039291382  val acc:  0.75\n",
      "epoch:  56  train loss:  0.8424814343452454  train acc:  0.9428571462631226  val loss:  1.2455006837844849  val acc:  0.7540000081062317\n",
      "epoch:  57  train loss:  0.8217083215713501  train acc:  0.949999988079071  val loss:  1.2288365364074707  val acc:  0.7580000162124634\n",
      "epoch:  58  train loss:  0.8217408061027527  train acc:  0.9071428775787354  val loss:  1.2125171422958374  val acc:  0.7620000243186951\n",
      "epoch:  59  train loss:  0.8100432753562927  train acc:  0.949999988079071  val loss:  1.1962497234344482  val acc:  0.765999972820282\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "optimizer = keras.optimizers.Adam(lr = 1e-2)\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pred, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.reduce_mean(keras.losses.categorical_crossentropy(y_train, gcn_model((features, graph_laplacian), training = True)[:y_train.shape[0]]))\n",
    "        train_acc = compute_accuracy(y_train, gcn_model((features, graph_laplacian))[:y_train.shape[0]])\n",
    "    grads = tape.gradient(loss, gcn_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, gcn_model.trainable_variables))\n",
    "\n",
    "    val_output = gcn_model((features, graph_laplacian), training = False)[y_train.shape[0]: y_train.shape[0] + 500]\n",
    "    val_loss = tf.reduce_mean(keras.losses.categorical_crossentropy(y_valid, val_output))\n",
    "    val_acc = compute_accuracy(y_valid, val_output)\n",
    "\n",
    "    print(\"epoch: \", epoch, \" train loss: \", float(loss.numpy()), \" train acc: \", float(train_acc.numpy()), \" val loss: \", float(val_loss.numpy()), \" val acc: \", float(val_acc.numpy()))\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test loss:  1.177369475364685  test_acc:  0.765999972820282\n"
     ]
    }
   ],
   "source": [
    "test_output = gcn_model((features, graph_laplacian), training = False)[-y_test.shape[0]:]\n",
    "test_loss = tf.reduce_mean(keras.losses.categorical_crossentropy(y_test, test_output))\n",
    "test_acc = compute_accuracy(y_test, test_output)\n",
    "\n",
    "print(\"test loss: \", float(test_loss.numpy()), \" test_acc: \", float(test_acc.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}