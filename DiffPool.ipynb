{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import dense_diff_pool, GCNConv, GraphConv, DenseGCNConv, JumpingKnowledge,DenseSAGEConv\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(777)\n",
    "else:\n",
    "    torch.manual_seed(777)"
   ]
  },
  {
   "source": [
    "# D&D"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.graphs = graphs\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.graphs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(data):\n",
    "    data.x = F.normalize(data.x, p=2,dim = -1)   # L_2归一化\n",
    "    return data\n",
    "\n",
    "DD = TUDataset(root='datasets/DD',name='DD', pre_transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n89\n1178\n"
     ]
    }
   ],
   "source": [
    "num_classes = DD.num_classes\n",
    "num_features = DD.num_features\n",
    "num_graphs = len(DD)\n",
    "print(num_classes)\n",
    "print(num_features)\n",
    "print(num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "num_train = int(num_graphs*0.8)\n",
    "num_val = int(num_graphs*0.1)\n",
    "num_test = num_graphs - (num_train+num_val)\n",
    "training_set, validation_set, testing_set = random_split(DD, [num_train, num_val, num_test])\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size= batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set,batch_size = batch_size, shuffle=False)\n",
    "test_loader = DataLoader(testing_set,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 1:\n=======\nNumber of graphs in the current batch: 32\nBatch(batch=[8211], edge_index=[2, 40850], x=[8211, 89], y=[32])\n\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import dense_diff_pool, SAGPooling"
   ]
  },
  {
   "source": [
    "## DiffPool on D&D "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`x`: $x \\in \\mathbb{R}^{B \\times N \\times F}$\n",
    "\n",
    "`adj`: $adj \\in \\mathbb{R}^{B \\times N \\times N}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5748\n"
     ]
    }
   ],
   "source": [
    "max_node = np.max([x.num_nodes for x in DD])\n",
    "ratio = 0.25  # Keep node ratio in each layer of DiffPool\n",
    "# learning_rate = 0.001\n",
    "reg = 0.0001\n",
    "epochs = 300\n",
    "num_hidden = 30\n",
    "print(max_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffPoolLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, num_cluster = 10):\n",
    "        super(DiffPoolLayer, self).__init__()\n",
    "        self.gnn_pool = DenseGCNConv(in_channel, num_cluster)  # GCN with mask\n",
    "        self.gnn_embed = DenseGCNConv(in_channel, out_channel)\n",
    "\n",
    "    def forward(self, x, adj, mask = None):\n",
    "        \"\"\"\n",
    "        x: feature matrix of batch graphs  x \\in (B,N,F)   B is batch size\n",
    "        adj: adj of batchs graphs        adj \\in (B,N,N) \n",
    "        batch: Batch vector, which assigns each node to a specific graph (0,0,0,0,1,1,1,...)\n",
    "        \"\"\"\n",
    "        S = F.relu(self.gnn_pool(x, adj, mask))\n",
    "        X = F.relu(self.gnn_embed(x, adj, mask))\n",
    "        X, adj, link_loss, ent_loss = dense_diff_pool(X,adj,S,mask)\n",
    "        return X, adj, link_loss+ent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_channel, Model, use_jumpingknowledge = False, dropout = .0):\n",
    "        super(Net, self).__init__()\n",
    "        ############################################### Out of Memory ###################################################\n",
    "        # self.conv1 = DenseGCNConv(num_features, hidden_channel)\n",
    "        # self.pool1 = DiffPoolLayer(hidden_channel, hidden_channel, np.ceil(ratio * max_node).astype(np.int))\n",
    "        \n",
    "        # self.conv2 = DenseGCNConv(hidden_channel, hidden_channel)\n",
    "        # self.pool2 = DiffPoolLayer(hidden_channel, hidden_channel, np.ceil(ratio ** 2 * max_node).astype(np.int))\n",
    "\n",
    "        # self.conv3 = DenseGCNConv(hidden_channel, hidden_channel)\n",
    "        # self.pool3 = DiffPoolLayer(hidden_channel, hidden_channel, np.ceil(ratio ** 3 * max_node).astype(np.int))\n",
    "        #################################################################################################################\n",
    "        self.conv1 = Model(num_features, hidden_channel)\n",
    "        self.conv2 = Model(hidden_channel, hidden_channel)\n",
    "        self.pool1 = DiffPoolLayer(hidden_channel, hidden_channel, np.ceil(ratio * max_node).astype(np.int))\n",
    "        self.conv3 = Model(hidden_channel, hidden_channel)\n",
    "        self.conv4 = Model(hidden_channel, hidden_channel)\n",
    "        ################################################################################################################# \n",
    "\n",
    "        self.use_jumpingknowledge = use_jumpingknowledge\n",
    "        if self.use_jumpingknowledge:\n",
    "            self.jump = JumpingKnowledge(mode='cat')\n",
    "        self.dropout = dropout\n",
    "        # self.linear1 = nn.Linear(3 * hidden_channel, hidden_channel)\n",
    "        self.linear2 = nn.Linear(hidden_channel, hidden_channel * 2)\n",
    "        self.linear3 = nn.Linear(hidden_channel * 2, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        data: a batch graphs like: Batch(batch=[33138], edge_index=[2, 164894], x=[33138, 89], y=[128])\n",
    "        \"\"\"\n",
    "        # batch, edge_index, x, y = data.batch, data.edge_index, data.x, data.y\n",
    "        # x, mask = to_dense_batch(x,batch)\n",
    "        # adj = to_dense_adj(edge_index, batch)\n",
    "        x, mask, adj = data[0], data[1],data[2]\n",
    "        xs = []\n",
    "        x = F.relu(self.conv1(x, adj, mask))\n",
    "        x = F.relu(self.conv2(x, adj, mask))\n",
    "\n",
    "        x, adj, reg = self.pool1(x, adj, mask)\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x, adj))\n",
    "        # x, adj, loss2 = self.pool2(x, adj)\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x, adj))\n",
    "        # x, adj, loss3 = self.pool3(x, adj)\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "        if self.use_jumpingknowledge:\n",
    "            x = self.jump(xs)\n",
    "            x = F.relu(self.linear1(x))\n",
    "        \n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        readout_x = torch.sum(x,dim = 1)\n",
    "        return readout_x , reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): DenseSAGEConv(89, 30)\n  (conv2): DenseSAGEConv(30, 30)\n  (pool1): DiffPoolLayer(\n    (gnn_pool): DenseGCNConv(30, 1437)\n    (gnn_embed): DenseGCNConv(30, 30)\n  )\n  (conv3): DenseSAGEConv(30, 30)\n  (conv4): DenseSAGEConv(30, 30)\n  (linear2): Linear(in_features=30, out_features=60, bias=True)\n  (linear3): Linear(in_features=60, out_features=2, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "model = Net(hidden_channel = num_hidden, Model=DenseSAGEConv)\n",
    "model.to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=reg)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        batch, edge_index, x, y = data.batch, data.edge_index, data.x, data.y\n",
    "        x, mask = to_dense_batch(x,batch)\n",
    "        adj = to_dense_adj(edge_index, batch)\n",
    "        data_input = [x, mask, adj]\n",
    "        output,reg = model(data_input)  # torch.Size([64, 2])\n",
    "\n",
    "        loss = loss_func(output, data.y) + reg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += (torch.argmax(output, -1) == data.y).sum().item()\n",
    "\n",
    "    acc_current_epoch = train_acc / len(loader.dataset)\n",
    "    return train_loss, acc_current_epoch\n",
    "    \n",
    "\n",
    "def val(loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        batch, edge_index, x, y = data.batch, data.edge_index, data.x, data.y\n",
    "\n",
    "        x, mask = to_dense_batch(x,batch)\n",
    "        adj = to_dense_adj(edge_index, batch)\n",
    "        data_input = [x, mask, adj]\n",
    "        output, _ = model(data_input)\n",
    "        correct += (torch.argmax(output,-1)==data.y).sum().item()\n",
    "        loss = loss_func(output, data.y)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    val_acc_epoch = correct / len(loader.dataset)\n",
    "    return val_loss, val_acc_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epochs:0 Train loss:199.64890241622925 Train accuracy:0.643312101910828 Validation loss:5.620843529701233 Validation accuracy:0.7863247863247863\n",
      "Epochs:1 Train loss:113.06811237335205 Train accuracy:0.7229299363057324 Validation loss:4.690021872520447 Validation accuracy:0.6666666666666666\n",
      "Epochs:2 Train loss:110.85566115379333 Train accuracy:0.6740976645435244 Validation loss:4.256055951118469 Validation accuracy:0.4358974358974359\n",
      "Epochs:3 Train loss:107.70110607147217 Train accuracy:0.6560509554140127 Validation loss:2.05905345082283 Validation accuracy:0.811965811965812\n",
      "Epochs:4 Train loss:104.84012830257416 Train accuracy:0.6942675159235668 Validation loss:1.8513012826442719 Validation accuracy:0.8205128205128205\n",
      "Epochs:5 Train loss:97.6878525018692 Train accuracy:0.6602972399150743 Validation loss:3.864527940750122 Validation accuracy:0.6581196581196581\n",
      "Epochs:6 Train loss:111.07568550109863 Train accuracy:0.6518046709129511 Validation loss:2.2108646631240845 Validation accuracy:0.7863247863247863\n",
      "Epochs:7 Train loss:97.60400748252869 Train accuracy:0.6825902335456475 Validation loss:2.1381828486919403 Validation accuracy:0.7264957264957265\n",
      "Epochs:8 Train loss:102.84472107887268 Train accuracy:0.6995753715498938 Validation loss:2.1800073385238647 Validation accuracy:0.7863247863247863\n",
      "Epochs:9 Train loss:100.59250283241272 Train accuracy:0.7016985138004246 Validation loss:1.9616953134536743 Validation accuracy:0.8034188034188035\n",
      "Epochs:10 Train loss:105.52882468700409 Train accuracy:0.6772823779193206 Validation loss:3.9340176582336426 Validation accuracy:0.6666666666666666\n",
      "Epochs:11 Train loss:106.62858784198761 Train accuracy:0.6698513800424628 Validation loss:4.919024705886841 Validation accuracy:0.4188034188034188\n",
      "Epochs:12 Train loss:102.23504519462585 Train accuracy:0.7229299363057324 Validation loss:2.0609737932682037 Validation accuracy:0.7692307692307693\n",
      "Epochs:13 Train loss:96.90896773338318 Train accuracy:0.7409766454352441 Validation loss:2.101672261953354 Validation accuracy:0.7350427350427351\n",
      "Epochs:14 Train loss:99.52334380149841 Train accuracy:0.7165605095541401 Validation loss:1.8344440162181854 Validation accuracy:0.811965811965812\n",
      "Epochs:15 Train loss:115.4616596698761 Train accuracy:0.6539278131634819 Validation loss:2.9574183225631714 Validation accuracy:0.7948717948717948\n",
      "Epochs:16 Train loss:101.18368685245514 Train accuracy:0.7101910828025477 Validation loss:2.0404531955718994 Validation accuracy:0.811965811965812\n",
      "Epochs:17 Train loss:98.49461388587952 Train accuracy:0.7197452229299363 Validation loss:1.7012533247470856 Validation accuracy:0.8205128205128205\n",
      "Epochs:18 Train loss:97.34436964988708 Train accuracy:0.7123142250530785 Validation loss:2.248416543006897 Validation accuracy:0.7264957264957265\n",
      "Epochs:19 Train loss:98.08127021789551 Train accuracy:0.7494692144373672 Validation loss:1.7451165318489075 Validation accuracy:0.8376068376068376\n",
      "Epochs:20 Train loss:96.61306035518646 Train accuracy:0.7176220806794055 Validation loss:2.235316812992096 Validation accuracy:0.7863247863247863\n",
      "Epochs:21 Train loss:103.13861584663391 Train accuracy:0.7112526539278131 Validation loss:2.267796516418457 Validation accuracy:0.8034188034188035\n",
      "Epochs:22 Train loss:97.64958775043488 Train accuracy:0.7462845010615711 Validation loss:3.623360514640808 Validation accuracy:0.6068376068376068\n",
      "Epochs:23 Train loss:105.59794116020203 Train accuracy:0.7006369426751592 Validation loss:2.3166941702365875 Validation accuracy:0.8034188034188035\n",
      "Epochs:24 Train loss:99.94962620735168 Train accuracy:0.7165605095541401 Validation loss:2.84582382440567 Validation accuracy:0.6410256410256411\n",
      "Epochs:25 Train loss:95.49303722381592 Train accuracy:0.7569002123142251 Validation loss:1.625923991203308 Validation accuracy:0.8290598290598291\n",
      "Epochs:26 Train loss:95.64720594882965 Train accuracy:0.7515923566878981 Validation loss:3.448943316936493 Validation accuracy:0.5982905982905983\n",
      "Epochs:27 Train loss:100.02902340888977 Train accuracy:0.6963906581740976 Validation loss:4.783432722091675 Validation accuracy:0.5982905982905983\n",
      "Epochs:28 Train loss:104.33817863464355 Train accuracy:0.692144373673036 Validation loss:2.807657301425934 Validation accuracy:0.7094017094017094\n",
      "Epochs:29 Train loss:102.6150329709053 Train accuracy:0.721868365180467 Validation loss:2.0180396139621735 Validation accuracy:0.7435897435897436\n",
      "Epochs:30 Train loss:96.49271512031555 Train accuracy:0.7314225053078556 Validation loss:5.679785251617432 Validation accuracy:0.5042735042735043\n",
      "Epochs:31 Train loss:103.12564206123352 Train accuracy:0.678343949044586 Validation loss:1.976061463356018 Validation accuracy:0.811965811965812\n",
      "Epochs:32 Train loss:103.39625334739685 Train accuracy:0.7165605095541401 Validation loss:2.1521331667900085 Validation accuracy:0.7264957264957265\n",
      "Epochs:33 Train loss:95.54144418239594 Train accuracy:0.7314225053078556 Validation loss:2.629145860671997 Validation accuracy:0.7094017094017094\n",
      "Epochs:34 Train loss:98.44510233402252 Train accuracy:0.7494692144373672 Validation loss:2.643446207046509 Validation accuracy:0.6666666666666666\n",
      "Epochs:35 Train loss:96.95201992988586 Train accuracy:0.7197452229299363 Validation loss:1.821512371301651 Validation accuracy:0.7948717948717948\n",
      "Epochs:36 Train loss:90.90357565879822 Train accuracy:0.7738853503184714 Validation loss:2.9759947061538696 Validation accuracy:0.6239316239316239\n",
      "Epochs:37 Train loss:100.5865707397461 Train accuracy:0.7399150743099787 Validation loss:1.663520634174347 Validation accuracy:0.8205128205128205\n",
      "Epochs:38 Train loss:94.05511689186096 Train accuracy:0.7866242038216561 Validation loss:1.9509753584861755 Validation accuracy:0.7777777777777778\n",
      "Epochs:39 Train loss:97.4789354801178 Train accuracy:0.7441613588110403 Validation loss:1.6217584908008575 Validation accuracy:0.811965811965812\n",
      "Epochs:40 Train loss:101.19183850288391 Train accuracy:0.7239915074309978 Validation loss:2.0920371413230896 Validation accuracy:0.8034188034188035\n",
      "Epochs:41 Train loss:96.91721665859222 Train accuracy:0.7717622080679406 Validation loss:1.7705420851707458 Validation accuracy:0.7863247863247863\n",
      "Epochs:42 Train loss:89.7388688325882 Train accuracy:0.7876857749469215 Validation loss:1.8295313715934753 Validation accuracy:0.7863247863247863\n",
      "Epochs:43 Train loss:96.48959290981293 Train accuracy:0.732484076433121 Validation loss:1.7267909348011017 Validation accuracy:0.8205128205128205\n",
      "Epochs:44 Train loss:94.89387285709381 Train accuracy:0.7685774946921444 Validation loss:2.574196219444275 Validation accuracy:0.7094017094017094\n",
      "Epochs:45 Train loss:98.37918376922607 Train accuracy:0.727176220806794 Validation loss:2.8667839765548706 Validation accuracy:0.7435897435897436\n",
      "Epochs:46 Train loss:102.4961531162262 Train accuracy:0.7229299363057324 Validation loss:2.6454317569732666 Validation accuracy:0.7606837606837606\n",
      "Epochs:47 Train loss:97.16248565912247 Train accuracy:0.7547770700636943 Validation loss:1.6212338507175446 Validation accuracy:0.8205128205128205\n",
      "Epochs:48 Train loss:94.3180103302002 Train accuracy:0.7834394904458599 Validation loss:1.643591046333313 Validation accuracy:0.8290598290598291\n",
      "Epochs:49 Train loss:91.97158885002136 Train accuracy:0.7802547770700637 Validation loss:1.7224738001823425 Validation accuracy:0.811965811965812\n",
      "Epochs:50 Train loss:99.29517495632172 Train accuracy:0.7282377919320594 Validation loss:1.8266855776309967 Validation accuracy:0.8290598290598291\n",
      "Epochs:51 Train loss:94.25202250480652 Train accuracy:0.7813163481953291 Validation loss:2.0740940868854523 Validation accuracy:0.7521367521367521\n",
      "Epochs:52 Train loss:96.3077621459961 Train accuracy:0.7409766454352441 Validation loss:3.517979383468628 Validation accuracy:0.6581196581196581\n",
      "Epochs:53 Train loss:93.96099817752838 Train accuracy:0.7377919320594479 Validation loss:6.223147392272949 Validation accuracy:0.5811965811965812\n",
      "Epochs:54 Train loss:105.12192928791046 Train accuracy:0.7208067940552016 Validation loss:1.9018047153949738 Validation accuracy:0.8205128205128205\n",
      "Epochs:55 Train loss:95.38878512382507 Train accuracy:0.7611464968152867 Validation loss:2.0782957673072815 Validation accuracy:0.811965811965812\n",
      "Epochs:56 Train loss:101.72229170799255 Train accuracy:0.7144373673036093 Validation loss:1.7673090100288391 Validation accuracy:0.8205128205128205\n",
      "Epochs:57 Train loss:95.86058628559113 Train accuracy:0.7388535031847133 Validation loss:1.8429742455482483 Validation accuracy:0.8205128205128205\n",
      "Epochs:58 Train loss:100.96541345119476 Train accuracy:0.7282377919320594 Validation loss:4.586134672164917 Validation accuracy:0.6581196581196581\n",
      "Epochs:59 Train loss:97.63192701339722 Train accuracy:0.7409766454352441 Validation loss:2.4015473127365112 Validation accuracy:0.7692307692307693\n",
      "Epochs:60 Train loss:89.23050308227539 Train accuracy:0.7749469214437368 Validation loss:2.7382304668426514 Validation accuracy:0.7350427350427351\n",
      "Epochs:61 Train loss:90.2528817653656 Train accuracy:0.7144373673036093 Validation loss:2.4239989519119263 Validation accuracy:0.7948717948717948\n",
      "Epochs:62 Train loss:83.19224321842194 Train accuracy:0.7409766454352441 Validation loss:6.215743660926819 Validation accuracy:0.5811965811965812\n",
      "Epochs:63 Train loss:61.448718309402466 Train accuracy:0.6815286624203821 Validation loss:1.6984094083309174 Validation accuracy:0.7948717948717948\n",
      "Epochs:64 Train loss:48.072927474975586 Train accuracy:0.7101910828025477 Validation loss:2.1802330911159515 Validation accuracy:0.8376068376068376\n",
      "Epochs:65 Train loss:43.869719326496124 Train accuracy:0.7016985138004246 Validation loss:1.9454686343669891 Validation accuracy:0.7777777777777778\n",
      "Epochs:66 Train loss:32.39259630441666 Train accuracy:0.732484076433121 Validation loss:3.7547245025634766 Validation accuracy:0.5811965811965812\n",
      "Epochs:67 Train loss:30.331993460655212 Train accuracy:0.7006369426751592 Validation loss:2.0817726254463196 Validation accuracy:0.7606837606837606\n",
      "Epochs:68 Train loss:33.08454787731171 Train accuracy:0.6613588110403397 Validation loss:6.21212112903595 Validation accuracy:0.5811965811965812\n",
      "Epochs:69 Train loss:36.74505537748337 Train accuracy:0.6910828025477707 Validation loss:4.582305192947388 Validation accuracy:0.5811965811965812\n",
      "Epochs:70 Train loss:29.17694067955017 Train accuracy:0.7144373673036093 Validation loss:1.807393878698349 Validation accuracy:0.8034188034188035\n",
      "Epochs:71 Train loss:26.034563899040222 Train accuracy:0.7377919320594479 Validation loss:1.9499143064022064 Validation accuracy:0.811965811965812\n",
      "Epochs:72 Train loss:26.414658427238464 Train accuracy:0.7346072186836518 Validation loss:1.6957804560661316 Validation accuracy:0.8205128205128205\n",
      "Epochs:73 Train loss:21.396826773881912 Train accuracy:0.721868365180467 Validation loss:1.8536779582500458 Validation accuracy:0.811965811965812\n",
      "Epochs:74 Train loss:19.470499485731125 Train accuracy:0.7611464968152867 Validation loss:2.716965079307556 Validation accuracy:0.6752136752136753\n",
      "Epochs:75 Train loss:22.73421347141266 Train accuracy:0.6963906581740976 Validation loss:1.7204334437847137 Validation accuracy:0.8205128205128205\n",
      "Epochs:76 Train loss:18.971088111400604 Train accuracy:0.7176220806794055 Validation loss:3.724849224090576 Validation accuracy:0.5897435897435898\n",
      "Epochs:77 Train loss:18.9726342856884 Train accuracy:0.7547770700636943 Validation loss:2.923484981060028 Validation accuracy:0.6495726495726496\n",
      "Epochs:78 Train loss:23.106017619371414 Train accuracy:0.6847133757961783 Validation loss:2.1576027572155 Validation accuracy:0.7521367521367521\n",
      "Epochs:79 Train loss:18.635210394859314 Train accuracy:0.7346072186836518 Validation loss:6.019197344779968 Validation accuracy:0.5811965811965812\n",
      "Epochs:80 Train loss:23.180392265319824 Train accuracy:0.6985138004246284 Validation loss:1.8665970265865326 Validation accuracy:0.7692307692307693\n",
      "Epochs:81 Train loss:23.690588146448135 Train accuracy:0.6878980891719745 Validation loss:2.9942779541015625 Validation accuracy:0.7008547008547008\n",
      "Epochs:82 Train loss:22.559362649917603 Train accuracy:0.7006369426751592 Validation loss:2.3352787494659424 Validation accuracy:0.7264957264957265\n",
      "Epochs:83 Train loss:17.436916768550873 Train accuracy:0.7622080679405521 Validation loss:1.7926005125045776 Validation accuracy:0.8205128205128205\n",
      "Epochs:84 Train loss:18.382153242826462 Train accuracy:0.7367303609341825 Validation loss:3.049013316631317 Validation accuracy:0.6666666666666666\n",
      "Epochs:85 Train loss:16.51174083352089 Train accuracy:0.7707006369426752 Validation loss:1.9441441297531128 Validation accuracy:0.7948717948717948\n",
      "Epochs:86 Train loss:16.575096130371094 Train accuracy:0.7579617834394905 Validation loss:2.2943188548088074 Validation accuracy:0.7264957264957265\n",
      "Epochs:87 Train loss:16.72427323460579 Train accuracy:0.7643312101910829 Validation loss:1.9311932921409607 Validation accuracy:0.8034188034188035\n",
      "Epochs:88 Train loss:17.19623112678528 Train accuracy:0.767515923566879 Validation loss:1.9294020235538483 Validation accuracy:0.7863247863247863\n",
      "Epochs:89 Train loss:19.678234547376633 Train accuracy:0.7303609341825902 Validation loss:1.8474412262439728 Validation accuracy:0.8290598290598291\n",
      "Epochs:90 Train loss:18.283110737800598 Train accuracy:0.7600849256900213 Validation loss:2.200853854417801 Validation accuracy:0.7435897435897436\n",
      "Epochs:91 Train loss:15.89821107685566 Train accuracy:0.778131634819533 Validation loss:2.0956701934337616 Validation accuracy:0.7435897435897436\n",
      "Epochs:92 Train loss:17.665019392967224 Train accuracy:0.7515923566878981 Validation loss:3.1518391370773315 Validation accuracy:0.6239316239316239\n",
      "Epochs:93 Train loss:19.650323420763016 Train accuracy:0.7367303609341825 Validation loss:5.165185928344727 Validation accuracy:0.5982905982905983\n",
      "Epochs:94 Train loss:21.982386261224747 Train accuracy:0.7377919320594479 Validation loss:4.006700396537781 Validation accuracy:0.6068376068376068\n",
      "Epochs:95 Train loss:18.229650795459747 Train accuracy:0.7367303609341825 Validation loss:2.794408082962036 Validation accuracy:0.6752136752136753\n",
      "Epochs:96 Train loss:16.060162648558617 Train accuracy:0.7802547770700637 Validation loss:2.0337444841861725 Validation accuracy:0.7777777777777778\n",
      "Epochs:97 Train loss:20.880091726779938 Train accuracy:0.7239915074309978 Validation loss:2.8734958171844482 Validation accuracy:0.717948717948718\n",
      "Epochs:98 Train loss:16.930764615535736 Train accuracy:0.7760084925690022 Validation loss:1.844433218240738 Validation accuracy:0.7948717948717948\n"
     ]
    }
   ],
   "source": [
    "patience = 0\n",
    "min_loss = 1e10\n",
    "limit_patience = 50\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    train_loss, acc_current_epoch = train(train_loader)\n",
    "    val_loss, val_acc_epoch = val(val_loader)\n",
    "    print(\"Epochs:{} Train loss:{} Train accuracy:{} Validation loss:{} Validation accuracy:{}\".format(epoch, train_loss, acc_current_epoch, val_loss, val_acc_epoch))\n",
    "    if val_loss < min_loss:\n",
    "        torch.save(model.state_dict(), 'saved_model')\n",
    "        min_loss = val_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    if patience > limit_patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc_epoch = val(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "62.006263660092365"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7815126050420168"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "test_acc_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}