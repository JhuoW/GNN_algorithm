{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import sys\n",
    "import os\n",
    "from utils import *\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0:3], device_type='GPU')"
   ]
  },
  {
   "source": [
    "This layer computes for each node \\(i\\):\n",
    "$$\n",
    "    \\Z_i = \\textrm{MLP}\\big( (1 + \\epsilon) \\cdot \\X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\X_j \\big)\n",
    "$$\n",
    "where MLP is a multi-layer perceptron."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csr_to_SparseTensor(csr_matrix):\n",
    "    if sp.isspmatrix_csr(csr_matrix):\n",
    "            csr_matrix = csr_matrix.tocoo()\n",
    "    row = csr_matrix.row\n",
    "    col = csr_matrix.col\n",
    "    pos = np.c_[row,col]\n",
    "    data = csr_matrix.data\n",
    "    shape = csr_matrix.shape\n",
    "    return tf.SparseTensor(pos, data, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cora\"\n",
    "\n",
    "adj, whole_features, whole_labels, train_idx, val_idx, test_idx = load_data(dataset_name)\n",
    "adj_SparseTensor = convert_csr_to_SparseTensor(adj)\n",
    "features = preprocess_features(whole_features).todense()\n",
    "\n",
    "num_nodes = features.shape[0]\n",
    "feature_dim = features.shape[1]\n",
    "epochs = 500\n",
    "n_classes = whole_labels.shape[1]\n",
    "dropout_rate = 0.6 \n",
    "learning_rate = 5e-3\n",
    "L2_reg = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLayer(keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 mlp_num, \n",
    "                 hidden_units,\n",
    "                 adj, \n",
    "                 num_nodes,\n",
    "                 epsilon=0,\n",
    "                 epsilon_learnable = True,\n",
    "                 mlp_activation = keras.activations.relu,\n",
    "                 activation = None,\n",
    "                 use_bias = False):\n",
    "        super(GINLayer,self).__init__(self)\n",
    "        self.output_dim = output_dim,\n",
    "        self.adj = adj\n",
    "        self.epsilon = epsilon\n",
    "        self.mlp_num = mlp_num\n",
    "        self.mlp_activation = mlp_activation\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.hidden_units = hidden_units  # 每个MLP的units数\n",
    "        self.epsilon_learnable = epsilon_learnable\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.hidden_units is not None and self.mlp_num > 0:\n",
    "            self.mlp = keras.models.Sequential([\n",
    "                keras.layers.Dense(units=units, activation=self.mlp_activation, use_bias=self.use_bias)\n",
    "                for units in self.hidden_units\n",
    "            ])\n",
    "        self.output_dense = keras.layers.Dense(units=self.output_dim[0], use_bias=self.use_bias, activation= self.activation)\n",
    "        if self.epsilon_learnable:\n",
    "            self.eps = self.add_weight(name = \"epsilon\",\n",
    "                            shape = (1,),\n",
    "                            initializer=keras.initializers.zeros,\n",
    "                            trainable=True)\n",
    "        else:\n",
    "            self.eps = tf.constant(self.epsilon)\n",
    "        self.built = True\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def _message_passing(self, adj, features):\n",
    "        index_i = adj.indices[:,0]\n",
    "        index_j = adj.indices[:,1]\n",
    "        messages = tf.gather(features, index_j)\n",
    "        agg = tf.math.unsorted_segment_sum(messages, index_i, self.num_nodes)\n",
    "        # agg = scatter_sum(messages, index_i, self.num_nodes)\n",
    "        return agg\n",
    "\n",
    "    def call(self, inputs):\n",
    "        features = inputs\n",
    "        assert keras.backend.is_sparse(self.adj), \"Adjcency Matrix Must be Sparse Tensor\"\n",
    "        assert keras.backend.ndim(features), \"Features Must be Rank 2\"\n",
    "        assert self.mlp_num == len(self.hidden_units) or self.hidden_units is None, \"Number of MLP layers must be equal to hidden_units list\"\n",
    "        agg = self._message_passing(self.adj, features)\n",
    "        if self.hidden_units is not None and self.mlp_num > 0:\n",
    "            mlp_output = self.mlp((1.+ self.eps) * features + agg)\n",
    "            out = self.output_dense(mlp_output)\n",
    "        else:\n",
    "            out = self.output_dense((1.+ self.eps) * features + agg)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 1433)]            0         \n_________________________________________________________________\ngin_layer (GINLayer)         (2708, 128)               199809    \n_________________________________________________________________\ngin_layer_1 (GINLayer)       (2708, 7)                 2161      \n=================================================================\nTotal params: 201,970\nTrainable params: 201,970\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_input = keras.Input(shape = (feature_dim,))\n",
    "# 第一层的MLP有两层\n",
    "gin1 = GINLayer(output_dim=128, \n",
    "                num_nodes = num_nodes,\n",
    "                mlp_num=1, \n",
    "                hidden_units=[128], \n",
    "                adj = adj_SparseTensor, \n",
    "                epsilon_learnable=True, \n",
    "                activation=keras.activations.relu)(model_input)\n",
    "gin2 = GINLayer(output_dim=n_classes,\n",
    "                num_nodes = num_nodes,\n",
    "                mlp_num=1, \n",
    "                hidden_units=[16], \n",
    "                adj=adj_SparseTensor,\n",
    "                epsilon_learnable=True, \n",
    "                activation=keras.activations.softmax)(gin1)\n",
    "# gin1 = GINLayer(output_dim=128, \n",
    "#                 mlp_num=0, \n",
    "#                 hidden_units=None, \n",
    "#                 adj = adj_SparseTensor, \n",
    "#                 epsilon_learnable=True, \n",
    "#                 activation=keras.activations.relu)(model_input)\n",
    "\n",
    "model = keras.models.Model(inputs = model_input, outputs = gin2)\n",
    "model.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              weighted_metrics=['categorical_crossentropy', 'acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c: 1.0000 - val_loss: 0.7048 - val_categorical_crossentropy: 3.8169 - val_acc: 0.7320\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.7549e-06 - categorical_crossentropy: 1.6934e-04 - acc: 1.0000 - val_loss: 0.7082 - val_categorical_crossentropy: 3.8355 - val_acc: 0.7320\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.5338e-06 - categorical_crossentropy: 1.6507e-04 - acc: 1.0000 - val_loss: 0.7117 - val_categorical_crossentropy: 3.8547 - val_acc: 0.7320\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.3186e-06 - categorical_crossentropy: 1.6091e-04 - acc: 1.0000 - val_loss: 0.7153 - val_categorical_crossentropy: 3.8741 - val_acc: 0.7320\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.1100e-06 - categorical_crossentropy: 1.5687e-04 - acc: 1.0000 - val_loss: 0.7189 - val_categorical_crossentropy: 3.8937 - val_acc: 0.7320\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.9083e-06 - categorical_crossentropy: 1.5297e-04 - acc: 1.0000 - val_loss: 0.7226 - val_categorical_crossentropy: 3.9134 - val_acc: 0.7320\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.7135e-06 - categorical_crossentropy: 1.4920e-04 - acc: 1.0000 - val_loss: 0.7262 - val_categorical_crossentropy: 3.9328 - val_acc: 0.7300\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5335e-06 - categorical_crossentropy: 1.4572e-04 - acc: 1.0000 - val_loss: 0.7297 - val_categorical_crossentropy: 3.9519 - val_acc: 0.7300\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 7.3735e-06 - categorical_crossentropy: 1.4262e-04 - acc: 1.0000 - val_loss: 0.7329 - val_categorical_crossentropy: 3.9694 - val_acc: 0.7300\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.2210e-06 - categorical_crossentropy: 1.3968e-04 - acc: 1.0000 - val_loss: 0.7358 - val_categorical_crossentropy: 3.9852 - val_acc: 0.7300\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.0748e-06 - categorical_crossentropy: 1.3685e-04 - acc: 1.0000 - val_loss: 0.7383 - val_categorical_crossentropy: 3.9988 - val_acc: 0.7300\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.9312e-06 - categorical_crossentropy: 1.3407e-04 - acc: 1.0000 - val_loss: 0.7404 - val_categorical_crossentropy: 4.0100 - val_acc: 0.7300\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.7882e-06 - categorical_crossentropy: 1.3130e-04 - acc: 1.0000 - val_loss: 0.7421 - val_categorical_crossentropy: 4.0190 - val_acc: 0.7300\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.6487e-06 - categorical_crossentropy: 1.2861e-04 - acc: 1.0000 - val_loss: 0.7434 - val_categorical_crossentropy: 4.0260 - val_acc: 0.7300\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.5151e-06 - categorical_crossentropy: 1.2602e-04 - acc: 1.0000 - val_loss: 0.7443 - val_categorical_crossentropy: 4.0311 - val_acc: 0.7300\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.3846e-06 - categorical_crossentropy: 1.2350e-04 - acc: 1.0000 - val_loss: 0.7450 - val_categorical_crossentropy: 4.0348 - val_acc: 0.7300\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.2585e-06 - categorical_crossentropy: 1.2106e-04 - acc: 1.0000 - val_loss: 0.7454 - val_categorical_crossentropy: 4.0373 - val_acc: 0.7340\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.1348e-06 - categorical_crossentropy: 1.1866e-04 - acc: 1.0000 - val_loss: 0.7458 - val_categorical_crossentropy: 4.0391 - val_acc: 0.7340\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.0130e-06 - categorical_crossentropy: 1.1631e-04 - acc: 1.0000 - val_loss: 0.7460 - val_categorical_crossentropy: 4.0403 - val_acc: 0.7340\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 5.8937e-06 - categorical_crossentropy: 1.1400e-04 - acc: 1.0000 - val_loss: 0.7462 - val_categorical_crossentropy: 4.0412 - val_acc: 0.7340\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 5.7783e-06 - categorical_crossentropy: 1.1177e-04 - acc: 1.0000 - val_loss: 0.7463 - val_categorical_crossentropy: 4.0419 - val_acc: 0.7340\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.6681e-06 - categorical_crossentropy: 1.0964e-04 - acc: 1.0000 - val_loss: 0.7464 - val_categorical_crossentropy: 4.0428 - val_acc: 0.7360\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.5645e-06 - categorical_crossentropy: 1.0763e-04 - acc: 1.0000 - val_loss: 0.7467 - val_categorical_crossentropy: 4.0440 - val_acc: 0.7340\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.4675e-06 - categorical_crossentropy: 1.0576e-04 - acc: 1.0000 - val_loss: 0.7469 - val_categorical_crossentropy: 4.0453 - val_acc: 0.7340\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.3746e-06 - categorical_crossentropy: 1.0396e-04 - acc: 1.0000 - val_loss: 0.7472 - val_categorical_crossentropy: 4.0469 - val_acc: 0.7340\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.2868e-06 - categorical_crossentropy: 1.0226e-04 - acc: 1.0000 - val_loss: 0.7476 - val_categorical_crossentropy: 4.0491 - val_acc: 0.7320\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 5.2048e-06 - categorical_crossentropy: 1.0068e-04 - acc: 1.0000 - val_loss: 0.7482 - val_categorical_crossentropy: 4.0521 - val_acc: 0.7320\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.1351e-06 - categorical_crossentropy: 9.9328e-05 - acc: 1.0000 - val_loss: 0.7490 - val_categorical_crossentropy: 4.0563 - val_acc: 0.7300\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.0680e-06 - categorical_crossentropy: 9.8030e-05 - acc: 1.0000 - val_loss: 0.7499 - val_categorical_crossentropy: 4.0617 - val_acc: 0.7300\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5.0015e-06 - categorical_crossentropy: 9.6744e-05 - acc: 1.0000 - val_loss: 0.7511 - val_categorical_crossentropy: 4.0680 - val_acc: 0.7300\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.9358e-06 - categorical_crossentropy: 9.5473e-05 - acc: 1.0000 - val_loss: 0.7525 - val_categorical_crossentropy: 4.0753 - val_acc: 0.7320\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 4.8717e-06 - categorical_crossentropy: 9.4232e-05 - acc: 1.0000 - val_loss: 0.7539 - val_categorical_crossentropy: 4.0833 - val_acc: 0.7320\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.8085e-06 - categorical_crossentropy: 9.3011e-05 - acc: 1.0000 - val_loss: 0.7555 - val_categorical_crossentropy: 4.0920 - val_acc: 0.7320\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.7470e-06 - categorical_crossentropy: 9.1821e-05 - acc: 1.0000 - val_loss: 0.7572 - val_categorical_crossentropy: 4.1010 - val_acc: 0.7320\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.6864e-06 - categorical_crossentropy: 9.0649e-05 - acc: 1.0000 - val_loss: 0.7589 - val_categorical_crossentropy: 4.1103 - val_acc: 0.7320\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.6273e-06 - categorical_crossentropy: 8.9506e-05 - acc: 1.0000 - val_loss: 0.7606 - val_categorical_crossentropy: 4.1195 - val_acc: 0.7320\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.5700e-06 - categorical_crossentropy: 8.8397e-05 - acc: 1.0000 - val_loss: 0.7623 - val_categorical_crossentropy: 4.1284 - val_acc: 0.7320\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.5143e-06 - categorical_crossentropy: 8.7320e-05 - acc: 1.0000 - val_loss: 0.7638 - val_categorical_crossentropy: 4.1369 - val_acc: 0.7320\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.4596e-06 - categorical_crossentropy: 8.6262e-05 - acc: 1.0000 - val_loss: 0.7652 - val_categorical_crossentropy: 4.1446 - val_acc: 0.7320\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.4065e-06 - categorical_crossentropy: 8.5234e-05 - acc: 1.0000 - val_loss: 0.7665 - val_categorical_crossentropy: 4.1514 - val_acc: 0.7320\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.3542e-06 - categorical_crossentropy: 8.4223e-05 - acc: 1.0000 - val_loss: 0.7676 - val_categorical_crossentropy: 4.1574 - val_acc: 0.7320\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.3026e-06 - categorical_crossentropy: 8.3224e-05 - acc: 1.0000 - val_loss: 0.7685 - val_categorical_crossentropy: 4.1623 - val_acc: 0.7320\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.2513e-06 - categorical_crossentropy: 8.2233e-05 - acc: 1.0000 - val_loss: 0.7693 - val_categorical_crossentropy: 4.1664 - val_acc: 0.7320\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 4.2014e-06 - categorical_crossentropy: 8.1267e-05 - acc: 1.0000 - val_loss: 0.7699 - val_categorical_crossentropy: 4.1697 - val_acc: 0.7320\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4.1527e-06 - categorical_crossentropy: 8.0326e-05 - acc: 1.0000 - val_loss: 0.7704 - val_categorical_crossentropy: 4.1725 - val_acc: 0.7320\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.1051e-06 - categorical_crossentropy: 7.9404e-05 - acc: 1.0000 - val_loss: 0.7708 - val_categorical_crossentropy: 4.1747 - val_acc: 0.7300\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.0586e-06 - categorical_crossentropy: 7.8504e-05 - acc: 1.0000 - val_loss: 0.7711 - val_categorical_crossentropy: 4.1764 - val_acc: 0.7300\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.0128e-06 - categorical_crossentropy: 7.7619e-05 - acc: 1.0000 - val_loss: 0.7714 - val_categorical_crossentropy: 4.1779 - val_acc: 0.7300\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.9677e-06 - categorical_crossentropy: 7.6747e-05 - acc: 1.0000 - val_loss: 0.7717 - val_categorical_crossentropy: 4.1793 - val_acc: 0.7280\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.9230e-06 - categorical_crossentropy: 7.5883e-05 - acc: 1.0000 - val_loss: 0.7719 - val_categorical_crossentropy: 4.1807 - val_acc: 0.7280\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.8789e-06 - categorical_crossentropy: 7.5029e-05 - acc: 1.0000 - val_loss: 0.7722 - val_categorical_crossentropy: 4.1822 - val_acc: 0.7280\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.8371e-06 - categorical_crossentropy: 7.4220e-05 - acc: 1.0000 - val_loss: 0.7725 - val_categorical_crossentropy: 4.1840 - val_acc: 0.7280\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.7958e-06 - categorical_crossentropy: 7.3421e-05 - acc: 1.0000 - val_loss: 0.7729 - val_categorical_crossentropy: 4.1859 - val_acc: 0.7280\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.7554e-06 - categorical_crossentropy: 7.2640e-05 - acc: 1.0000 - val_loss: 0.7733 - val_categorical_crossentropy: 4.1881 - val_acc: 0.7280\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.7155e-06 - categorical_crossentropy: 7.1868e-05 - acc: 1.0000 - val_loss: 0.7737 - val_categorical_crossentropy: 4.1906 - val_acc: 0.7280\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.6766e-06 - categorical_crossentropy: 7.1116e-05 - acc: 1.0000 - val_loss: 0.7742 - val_categorical_crossentropy: 4.1933 - val_acc: 0.7280\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 3.6388e-06 - categorical_crossentropy: 7.0385e-05 - acc: 1.0000 - val_loss: 0.7748 - val_categorical_crossentropy: 4.1964 - val_acc: 0.7280\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.6017e-06 - categorical_crossentropy: 6.9668e-05 - acc: 1.0000 - val_loss: 0.7754 - val_categorical_crossentropy: 4.1997 - val_acc: 0.7280\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.5648e-06 - categorical_crossentropy: 6.8954e-05 - acc: 1.0000 - val_loss: 0.7761 - val_categorical_crossentropy: 4.2033 - val_acc: 0.7280\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.5288e-06 - categorical_crossentropy: 6.8257e-05 - acc: 1.0000 - val_loss: 0.7768 - val_categorical_crossentropy: 4.2072 - val_acc: 0.7300\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.4933e-06 - categorical_crossentropy: 6.7570e-05 - acc: 1.0000 - val_loss: 0.7776 - val_categorical_crossentropy: 4.2113 - val_acc: 0.7300\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.4582e-06 - categorical_crossentropy: 6.6892e-05 - acc: 1.0000 - val_loss: 0.7784 - val_categorical_crossentropy: 4.2156 - val_acc: 0.7300\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.4238e-06 - categorical_crossentropy: 6.6225e-05 - acc: 1.0000 - val_loss: 0.7791 - val_categorical_crossentropy: 4.2199 - val_acc: 0.7300\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.3902e-06 - categorical_crossentropy: 6.5576e-05 - acc: 1.0000 - val_loss: 0.7799 - val_categorical_crossentropy: 4.2242 - val_acc: 0.7300\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.3573e-06 - categorical_crossentropy: 6.4940e-05 - acc: 1.0000 - val_loss: 0.7807 - val_categorical_crossentropy: 4.2284 - val_acc: 0.7300\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.3250e-06 - categorical_crossentropy: 6.4314e-05 - acc: 1.0000 - val_loss: 0.7815 - val_categorical_crossentropy: 4.2326 - val_acc: 0.7300\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.2930e-06 - categorical_crossentropy: 6.3697e-05 - acc: 1.0000 - val_loss: 0.7823 - val_categorical_crossentropy: 4.2367 - val_acc: 0.7300\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 3.2607e-06 - categorical_crossentropy: 6.3072e-05 - acc: 1.0000 - val_loss: 0.7830 - val_categorical_crossentropy: 4.2408 - val_acc: 0.7320\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.2299e-06 - categorical_crossentropy: 6.2476e-05 - acc: 1.0000 - val_loss: 0.7837 - val_categorical_crossentropy: 4.2447 - val_acc: 0.7320\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1995e-06 - categorical_crossentropy: 6.1888e-05 - acc: 1.0000 - val_loss: 0.7844 - val_categorical_crossentropy: 4.2485 - val_acc: 0.7320\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1695e-06 - categorical_crossentropy: 6.1308e-05 - acc: 1.0000 - val_loss: 0.7851 - val_categorical_crossentropy: 4.2522 - val_acc: 0.7320\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1400e-06 - categorical_crossentropy: 6.0736e-05 - acc: 1.0000 - val_loss: 0.7858 - val_categorical_crossentropy: 4.2556 - val_acc: 0.7300\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1106e-06 - categorical_crossentropy: 6.0169e-05 - acc: 1.0000 - val_loss: 0.7864 - val_categorical_crossentropy: 4.2589 - val_acc: 0.7300\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0819e-06 - categorical_crossentropy: 5.9612e-05 - acc: 1.0000 - val_loss: 0.7869 - val_categorical_crossentropy: 4.2620 - val_acc: 0.7300\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0535e-06 - categorical_crossentropy: 5.9063e-05 - acc: 1.0000 - val_loss: 0.7875 - val_categorical_crossentropy: 4.2651 - val_acc: 0.7300\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0256e-06 - categorical_crossentropy: 5.8523e-05 - acc: 1.0000 - val_loss: 0.7880 - val_categorical_crossentropy: 4.2679 - val_acc: 0.7280\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9978e-06 - categorical_crossentropy: 5.7987e-05 - acc: 1.0000 - val_loss: 0.7885 - val_categorical_crossentropy: 4.2707 - val_acc: 0.7280\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9705e-06 - categorical_crossentropy: 5.7459e-05 - acc: 1.0000 - val_loss: 0.7890 - val_categorical_crossentropy: 4.2734 - val_acc: 0.7300\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9441e-06 - categorical_crossentropy: 5.6947e-05 - acc: 1.0000 - val_loss: 0.7895 - val_categorical_crossentropy: 4.2760 - val_acc: 0.7300\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9176e-06 - categorical_crossentropy: 5.6434e-05 - acc: 1.0000 - val_loss: 0.7900 - val_categorical_crossentropy: 4.2785 - val_acc: 0.7300\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 2.8914e-06 - categorical_crossentropy: 5.5928e-05 - acc: 1.0000 - val_loss: 0.7904 - val_categorical_crossentropy: 4.2810 - val_acc: 0.7300\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8658e-06 - categorical_crossentropy: 5.5432e-05 - acc: 1.0000 - val_loss: 0.7909 - val_categorical_crossentropy: 4.2835 - val_acc: 0.7300\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8408e-06 - categorical_crossentropy: 5.4950e-05 - acc: 1.0000 - val_loss: 0.7914 - val_categorical_crossentropy: 4.2860 - val_acc: 0.7300\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8157e-06 - categorical_crossentropy: 5.4463e-05 - acc: 1.0000 - val_loss: 0.7918 - val_categorical_crossentropy: 4.2885 - val_acc: 0.7300\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7912e-06 - categorical_crossentropy: 5.3989e-05 - acc: 1.0000 - val_loss: 0.7923 - val_categorical_crossentropy: 4.2910 - val_acc: 0.7300\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7672e-06 - categorical_crossentropy: 5.3525e-05 - acc: 1.0000 - val_loss: 0.7928 - val_categorical_crossentropy: 4.2936 - val_acc: 0.7300\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7433e-06 - categorical_crossentropy: 5.3063e-05 - acc: 1.0000 - val_loss: 0.7933 - val_categorical_crossentropy: 4.2963 - val_acc: 0.7300\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7193e-06 - categorical_crossentropy: 5.2599e-05 - acc: 1.0000 - val_loss: 0.7938 - val_categorical_crossentropy: 4.2990 - val_acc: 0.7300\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.6962e-06 - categorical_crossentropy: 5.2152e-05 - acc: 1.0000 - val_loss: 0.7943 - val_categorical_crossentropy: 4.3017 - val_acc: 0.7300\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6733e-06 - categorical_crossentropy: 5.1710e-05 - acc: 1.0000 - val_loss: 0.7948 - val_categorical_crossentropy: 4.3045 - val_acc: 0.7300\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 2.6508e-06 - categorical_crossentropy: 5.1274e-05 - acc: 1.0000 - val_loss: 0.7953 - val_categorical_crossentropy: 4.3074 - val_acc: 0.7280\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6284e-06 - categorical_crossentropy: 5.0840e-05 - acc: 1.0000 - val_loss: 0.7959 - val_categorical_crossentropy: 4.3103 - val_acc: 0.7280\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6061e-06 - categorical_crossentropy: 5.0409e-05 - acc: 1.0000 - val_loss: 0.7964 - val_categorical_crossentropy: 4.3133 - val_acc: 0.7280\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5846e-06 - categorical_crossentropy: 4.9994e-05 - acc: 1.0000 - val_loss: 0.7970 - val_categorical_crossentropy: 4.3163 - val_acc: 0.7300\n"
     ]
    }
   ],
   "source": [
    "y_train, train_mask = generate_mask_data(whole_labels, train_idx)\n",
    "y_val, val_mask = generate_mask_data(whole_labels, val_idx)\n",
    "y_test, test_mask = generate_mask_data(whole_labels, test_idx)\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x =  features, \n",
    "                         y = y_train, \n",
    "                         sample_weight= train_mask,\n",
    "                         validation_data=(features, y_val, val_mask),\n",
    "                         batch_size = num_nodes, epochs = 300, \n",
    "                         shuffle = False, \n",
    "                         workers=10, \n",
    "                         use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5001 - categorical_crossentropy: 4.0622 - acc: 0.7390\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate(features, y_test, sample_weight=test_mask, batch_size=num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}