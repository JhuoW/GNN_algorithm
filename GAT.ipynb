{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import  pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1, 2'\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[1:3], device_type='GPU')"
   ]
  },
  {
   "source": [
    "# DataLoad\n",
    "\n",
    "without sparse operation in this project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cora\"\n",
    "\n",
    "def preprocess_features(features):\n",
    "    row_sum = np.array(features.sum(1))\n",
    "    reverse_row_sum = np.power(row_sum,-1).flatten()\n",
    "    reverse_row_sum[np.isinf(reverse_row_sum)] = 0.\n",
    "    new_features = sp.diags(reverse_row_sum).dot(features)\n",
    "    return new_features\n",
    "\n",
    "def load_data(datasetname):\n",
    "    names = ['x','tx','allx','y','ty','ally','graph']\n",
    "    objects = {}\n",
    "    for name in names:\n",
    "        with open(\"data/ind.{}.{}\".format(datasetname, name),'rb') as f:\n",
    "            objects[name] = pkl.load(f, encoding='latin1')\n",
    "    \n",
    "    with open(\"data/ind.{}.test.index\".format(datasetname), 'r') as f:\n",
    "        test_index = []\n",
    "        for line in f.readlines():\n",
    "            test_index.append(int(line.strip()))\n",
    "\n",
    "    test_index_reorder = np.sort(test_index)\n",
    "    \n",
    "    whole_features = sp.vstack((objects['allx'], objects['tx'])).tolil()\n",
    "\n",
    "    whole_features[test_index] = whole_features[test_index_reorder]\n",
    "\n",
    "    num_nodes = whole_features.shape[0]\n",
    "\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(objects['graph']))\n",
    "\n",
    "    whole_labels = np.r_[objects['ally'], objects['ty']]\n",
    "\n",
    "    whole_labels[test_index] = whole_labels[test_index_reorder]\n",
    "\n",
    "    train_idx = np.arange(len(objects['y']))\n",
    "    val_idx = np.arange(len(objects['y']), len(objects['y'])+ 500)\n",
    "    test_idx = test_index_reorder\n",
    "\n",
    "\n",
    "\n",
    "    return adj, whole_features, whole_labels, train_idx, val_idx, test_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, whole_features, whole_labels, train_idx, val_idx, test_idx = load_data(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "features = preprocess_features(whole_features).todense()"
   ]
  },
  {
   "source": [
    "# 创建GAT Layer\n",
    "\n",
    "$L2_{reg} = \\frac{\\lambda}{2m}||W||^2_2$\n",
    "\n",
    "$a^T[h_iW||h_jW] = <a^T_1,h_iW> + <a^T_2, h_jW>$  其中， $a = concat(a_1, a_2)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = features.shape[0]\n",
    "feature_dim = features.shape[1]\n",
    "epochs = 500\n",
    "n_classes = whole_labels.shape[1]\n",
    "n_att_heads = 8\n",
    "feature_dim_each_head = 8\n",
    "dropout_rate = 0.6  # for input and normalized attention coefficients \\alpha in paper\n",
    "learning_rate = 5e-3\n",
    "L2_reg = 5e-4\n",
    "\n",
    "class GATlayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    GATLayer为GAT的层， 每层的输入为:\n",
    "    output_dims: dim of the output of the current layer  (F' in the paper)\n",
    "    dropout: both to input and normalized attention coefficient (训练阶段，节点会随机采样邻居)\n",
    "    n_heads: number of attention head\n",
    "    aggregation: {avg, concat}\n",
    "    activation: elu in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dims, \n",
    "                       num_nodes, \n",
    "                       dropout = 0.6, \n",
    "                       n_heads = 8, \n",
    "                       activation = None,  \n",
    "                       aggregation = \"avg\",  \n",
    "                       use_bias = False,\n",
    "                       L2_reg = 5e-3,\n",
    "                       **kwargs):\n",
    "\n",
    "        self.output_dims = output_dims\n",
    "        self.num_nodes = num_nodes\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.aggregation = aggregation\n",
    "        self.L2_reg = L2_reg\n",
    "        self.use_bias = use_bias\n",
    "        super(GATlayer,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        input: all features and adj\n",
    "        input_shape: [(X.shape), (A.shape)]\n",
    "        X.shape = (None,1433)\n",
    "        A.shape = (None,2708)\n",
    "        \"\"\"\n",
    "        print(input_shape)\n",
    "        self.all_feature_transform_weights = []\n",
    "        self.all_feature_transform_bias = []\n",
    "        self.all_attention = []\n",
    "        for head in range(self.n_heads):\n",
    "            weights_feature_transform = self.add_weight(name = \"Weight_{}\".format(head),\n",
    "                                                             shape = (input_shape[0][1], self.output_dims),  \n",
    "                                                             initializer = keras.initializers.glorot_uniform,\n",
    "                                                             regularizer = keras.regularizers.L2(self.L2_reg),\n",
    "                                                             trainable=True)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                bias_feature_transform = self.add_weight(name = \"bias_{}\".format(head),\n",
    "                                                              shape = (self.output_dims,1),\n",
    "                                                              initializer = keras.initializers.zeros,\n",
    "                                                              trainable=True)\n",
    "                self.all_feature_transform_bias.append(bias_feature_transform)\n",
    "            \n",
    "            self.all_feature_transform_weights.append(weights_feature_transform)\n",
    "            \n",
    "\n",
    "        \n",
    "            attention_weight_self = self.add_weight(name = \"att_{}_self\".format(head),\n",
    "                                                    shape = (self.output_dims,1),\n",
    "                                                    initializer = keras.initializers.glorot_uniform,\n",
    "                                                    regularizer = keras.regularizers.L2(self.L2_reg),\n",
    "                                                    trainable=True)\n",
    "            attention_weight_neigh = self.add_weight(name = \"att_{}_neigh\".format(head),\n",
    "                                                          shape=(self.output_dims,1),\n",
    "                                                          initializer= keras.initializers.glorot_uniform,\n",
    "                                                          regularizer=keras.regularizers.L2(self.L2_reg),\n",
    "                                                          trainable=True)\n",
    "\n",
    "            self.all_attention.append([attention_weight_self, attention_weight_neigh])\n",
    "        super(GATlayer,self).build(input_shape)\n",
    "\n",
    "    \n",
    "    def _toSparseTensor(self,X):\n",
    "        \"\"\"\n",
    "        X is a dense np matrix\n",
    "        \"\"\"\n",
    "        idx = tf.where(tf.not_equal(X, 0))\n",
    "        sparse = tf.SparseTensor(indices=idx, values = tf.gather_nd(X,idx), dense_shape=X.shape)\n",
    "        return sparse\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: [features, adj]\n",
    "        \"\"\"\n",
    "        assert self.aggregation == \"avg\" or self.aggregation == \"concat\", \"Aggregation must be 'concat' or 'avg'\"\n",
    "        features = inputs[0]\n",
    "        # sparse_features = self._toSparseTensor(features)\n",
    "        adj = inputs[1]\n",
    "        # sparse_adj = self._toSparseTensor(adj)\n",
    "        output = []\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            feature_transform = tf.matmul(features, self.all_feature_transform_weights[head])  # 为每个节点做特征变换 hW  (N,F')\n",
    "\n",
    "            # a = [a_1,a_2]  a.shape (2*F', 1)\n",
    "            attention_to_center = tf.matmul(feature_transform, self.all_attention[head][0])   # 所有 h_iWa_1  (None, 1)\n",
    "            \n",
    "            attention_to_neighbor = tf.matmul(feature_transform, self.all_attention[head][1])   # (None, 1)\n",
    "\n",
    "            # compute eq.1 in the paper, to obtain a matrix\n",
    "            # 计算每个节点和和其他节点之间的attention系数， 1. 特征变换， 2. 拼接， 3. 单层前馈 \n",
    "            attention_to_center = tf.tile(attention_to_center, multiples=[1, self.num_nodes])  \n",
    "\n",
    "            att_coef_matrix = tf.add(attention_to_center, tf.transpose(attention_to_neighbor))  # (N,N)  xx\n",
    "\n",
    "            att_coef_mask = keras.layers.LeakyReLU(alpha=0.2)(att_coef_matrix)\n",
    "\n",
    "            # neighbor mask\n",
    "            # att_coef_mask = tf.multiply(att_coef_matrix, adj)  将非邻居置为0（错误） 应置为-inf  应为exp(-inf) = 0\n",
    "            mask = -10e9 * (1.0 - adj)\n",
    "            att_coef_mask = att_coef_matrix + mask\n",
    "\n",
    "            # eq3:\n",
    "            alpha_matrix = keras.activations.softmax(att_coef_mask)\n",
    "            \n",
    "            # dropout for input feature and attention matrix\n",
    "            # dropout_feature = keras.layers.Dropout(self.dropout)(feature_transform)  # 一部分置0  (N,F')\n",
    "            # dropout_att = keras.layers.Dropout(self.dropout)(alpha_matrix)  # 一部分置0 文中解释：聚合时随机采样邻居 (N,N)\n",
    "\n",
    "            # eq4:\n",
    "            new_node_features = tf.matmul(alpha_matrix, feature_transform)    # (N,F')\n",
    "\n",
    "            if self.use_bias:\n",
    "                bias = self.all_feature_transform_bias[head]   # (F',1)\n",
    "                new_node_features = tf.nn.bias_add(new_node_features, tf.squeeze(tf.reshape(bias,shape=(1,-1)))  ) # (N,F')\n",
    "            \n",
    "\n",
    "            # 将一个head的node features保存\n",
    "            output.append(new_node_features) \n",
    "        \n",
    "        if self.aggregation == \"avg\":\n",
    "            out = tf.reduce_mean(output,axis = 0)\n",
    "        else:\n",
    "            out = tf.concat(output, axis = 1)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[TensorShape([None, 1433]), TensorShape([None, 2708])]\n",
      "[TensorShape([None, 64]), TensorShape([None, 2708])]\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1433)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1433)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2708)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ga_tlayer (GATlayer)            (None, 64)           91840       dropout[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           ga_tlayer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ga_tlayer_1 (GATlayer)          (None, 7)            462         dropout_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 92,302\n",
      "Trainable params: 92,302\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_features = keras.Input(shape = (features.shape[1],))  \n",
    "input_adj = keras.Input(shape = (adj.shape[1],))\n",
    "dropout1 = keras.layers.Dropout(dropout_rate)(input_features)\n",
    "GAT1 = GATlayer(output_dims = feature_dim_each_head,  # 8\n",
    "                num_nodes = num_nodes,\n",
    "                dropout=dropout_rate, \n",
    "                n_heads = n_att_heads, \n",
    "                activation = keras.activations.elu,\n",
    "                aggregation=\"concat\",\n",
    "                use_bias=False, \n",
    "                L2_reg=L2_reg)([dropout1, input_adj])\n",
    "\n",
    "dropout2 = keras.layers.Dropout(dropout_rate)(GAT1)\n",
    "\n",
    "GAT2 = GATlayer(output_dims=n_classes,\n",
    "                num_nodes = num_nodes,\n",
    "                dropout = dropout_rate, \n",
    "                n_heads = 1, \n",
    "                activation = keras.activations.softmax,\n",
    "                aggregation='avg',\n",
    "                use_bias = False,\n",
    "                L2_reg=L2_reg)([dropout2,input_adj])\n",
    "\n",
    "model = keras.models.Model(inputs = [input_features, input_adj], outputs = GAT2)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss = keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = adj + np.eye(num_nodes)\n",
    "A = adj.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 109ms/step - loss: 0.1006 - accuracy: 0.9140 - val_loss: 0.3593 - val_accuracy: 0.8220\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1006 - accuracy: 0.9313 - val_loss: 0.3593 - val_accuracy: 0.8371\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1006 - accuracy: 0.9369 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1006 - accuracy: 0.9369 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1006 - accuracy: 0.9369 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1006 - accuracy: 0.9350 - val_loss: 0.3593 - val_accuracy: 0.8375\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1006 - accuracy: 0.9317 - val_loss: 0.3593 - val_accuracy: 0.8371\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1006 - accuracy: 0.9346 - val_loss: 0.3593 - val_accuracy: 0.8357\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1006 - accuracy: 0.9250 - val_loss: 0.3593 - val_accuracy: 0.8312\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1006 - accuracy: 0.9372 - val_loss: 0.3593 - val_accuracy: 0.8287\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1006 - accuracy: 0.9424 - val_loss: 0.3593 - val_accuracy: 0.8283\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1006 - accuracy: 0.9369 - val_loss: 0.3593 - val_accuracy: 0.8242\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9450 - val_loss: 0.3593 - val_accuracy: 0.8253\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9335 - val_loss: 0.3593 - val_accuracy: 0.8224\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9354 - val_loss: 0.3593 - val_accuracy: 0.8213\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1006 - accuracy: 0.9391 - val_loss: 0.3593 - val_accuracy: 0.8047\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1006 - accuracy: 0.9317 - val_loss: 0.3593 - val_accuracy: 0.7936\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1006 - accuracy: 0.9324 - val_loss: 0.3593 - val_accuracy: 0.7891\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1006 - accuracy: 0.9295 - val_loss: 0.3593 - val_accuracy: 0.7869\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1006 - accuracy: 0.9346 - val_loss: 0.3593 - val_accuracy: 0.7747\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1006 - accuracy: 0.9309 - val_loss: 0.3593 - val_accuracy: 0.7688\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1006 - accuracy: 0.9195 - val_loss: 0.3593 - val_accuracy: 0.7729\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1006 - accuracy: 0.9247 - val_loss: 0.3593 - val_accuracy: 0.7903\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1006 - accuracy: 0.9339 - val_loss: 0.3593 - val_accuracy: 0.8087\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1006 - accuracy: 0.9343 - val_loss: 0.3593 - val_accuracy: 0.8239\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1006 - accuracy: 0.9409 - val_loss: 0.3593 - val_accuracy: 0.8353\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1006 - accuracy: 0.9461 - val_loss: 0.3593 - val_accuracy: 0.8375\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1006 - accuracy: 0.9513 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1006 - accuracy: 0.9538 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1006 - accuracy: 0.9531 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9546 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1006 - accuracy: 0.9531 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1006 - accuracy: 0.9538 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9535 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1006 - accuracy: 0.9535 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1006 - accuracy: 0.9516 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1006 - accuracy: 0.9542 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1006 - accuracy: 0.9527 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9546 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1006 - accuracy: 0.9546 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1006 - accuracy: 0.9542 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1006 - accuracy: 0.9561 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1006 - accuracy: 0.9561 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1006 - accuracy: 0.9561 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1006 - accuracy: 0.9542 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1006 - accuracy: 0.9561 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1006 - accuracy: 0.9564 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1006 - accuracy: 0.9542 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9542 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1006 - accuracy: 0.9531 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1006 - accuracy: 0.9561 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1006 - accuracy: 0.9564 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1006 - accuracy: 0.9549 - val_loss: 0.3593 - val_accuracy: 0.8383\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1006 - accuracy: 0.9524 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1006 - accuracy: 0.9546 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1006 - accuracy: 0.9546 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1006 - accuracy: 0.9535 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1006 - accuracy: 0.9561 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9564 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1006 - accuracy: 0.9549 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9542 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1006 - accuracy: 0.9553 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1006 - accuracy: 0.9557 - val_loss: 0.3593 - val_accuracy: 0.8379\n"
     ]
    }
   ],
   "source": [
    "# model([features,A])\n",
    "def mask(X, ids):\n",
    "    mask_X = np.zeros(shape = X.shape)\n",
    "    mask = np.zeros(shape = X.shape[0]).astype(np.bool)\n",
    "    mask[ids] = True\n",
    "    mask_X[ids] = X[mask]\n",
    "    return mask_X\n",
    "\n",
    "history = model.fit([mask(features, train_idx),A], \n",
    "                    mask(whole_labels,train_idx),\n",
    "                    epochs=epochs, \n",
    "                    # sample_weight = mask(np.ones(num_nodes), train_idx),\n",
    "                    batch_size = num_nodes,\n",
    "                    shuffle=False, \n",
    "                    validation_data=([mask(features,val_idx), A], mask(whole_labels, val_idx)),\n",
    "                    workers=10, use_multiprocessing=True)\n",
    "                                                                                "
   ]
  },
  {
   "source": [
    "# Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1006 - accuracy: 0.9557\n"
     ]
    }
   ],
   "source": [
    "eval_test = model.evaluate([mask(features,test_idx), A], mask(whole_labels,test_idx), batch_size=num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}